{"cells":[{"cell_type":"markdown","metadata":{"id":"jXpPuOZ53Hxz"},"source":["# 0. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25439,"status":"ok","timestamp":1757899718286,"user":{"displayName":"Di Lab","userId":"16690045528125560783"},"user_tz":-540},"id":"t713PxW-aTH2","outputId":"dc95d43e-ff57-4691-a22e-27b3af19872e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"BoUUKt1R3IEL"},"source":["# 1. ì…ë ¥ ë°ì´í„°ì…‹ êµ¬ì¶•"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"4wDMNegR3ITi","outputId":"1447adf3-2a14-4421-ef73-24408359356a"},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ“¥ Loading patients/admissions ...\n","âœ… core rows: 3\n","ğŸ“¥ Loading prescriptions ...\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-510581600.py:109: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda g: pd.Series({\"inpatient_med_summary_json\": one_hadm(g)}))\n"]},{"name":"stdout","output_type":"stream","text":["ğŸ“¥ Loading d_labitems & labevents (filtered) ...\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-510581600.py:199: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda g: pd.Series({\"lab_summary_json\": one_hadm(g)}))\n"]},{"name":"stdout","output_type":"stream","text":["ğŸ“¥ Loading microbiologyevents ...\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-510581600.py:249: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda g: pd.Series({\"microbio_summary_json\": summarize_micro(g)}))\n"]},{"name":"stdout","output_type":"stream","text":["ğŸ“¥ Loading icustays (light) ...\n","ğŸ§© Assembling (A-input) ...\n","âœ… A-input rows: 3, cols: 17\n","ğŸ’¾ Saved Parquet â†’ /content/drive/MyDrive/DILAB/MARS/mimic-iv_reconstructed/mimiciv_A_input_struct_only.parquet\n","ğŸ‰ Done (A-input)!\n"]}],"source":["# === 0) ê¸°ë³¸ ì„¸íŒ… ============================================================\n","import os, json, gc\n","import pandas as pd\n","import numpy as np\n","\n","# === 1) ê²½ë¡œ/ì˜µì…˜ =============================================================\n","HOSP_DIR   = \"/content/drive/MyDrive/DILAB/MARS/mimiciv_3.1/files/hosp\"\n","ICU_DIR    = \"/content/drive/MyDrive/DILAB/MARS/mimiciv_3.1/files/icu\"\n","NOTE_DIR   = \"/content/drive/MyDrive/DILAB/MARS/mimic-iv-note_2.2/files/note\"\n","\n","OUTPUT_DIR = \"/content/drive/MyDrive/DILAB/MARS/mimic-iv_reconstructed\"\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","# í…ŒìŠ¤íŠ¸/ì„±ëŠ¥ ì˜µì…˜\n","HADM_LIMIT                 = None     # ê²€ì¦ í›„ None\n","USE_EMAR                   = False\n","USE_LABS                   = True\n","USE_MICROBIO               = True\n","USE_ICU_LIGHT              = True\n","# âš ï¸ A-êµ¬ì¡° ì…ë ¥ì—ì„œëŠ” í…ìŠ¤íŠ¸ ìŠ¤ë‹ˆí« ë¶ˆí•„ìš”\n","USE_RADIOLOGY_TEXT         = False\n","USE_ICU_VENT_PRESSOR_HEAVY = False\n","\n","SAVE_PARQUET = True\n","SAVE_CSV     = False\n","\n","# ìœ í‹¸\n","def td(dt):  # ì•ˆì „í•œ to_datetime\n","    return pd.to_datetime(dt, errors=\"coerce\", utc=True)\n","\n","def jdump(x):\n","    return json.dumps(x, ensure_ascii=False, separators=(\",\", \":\"))\n","\n","def safe_read_csv(path, usecols=None, dtype=None):\n","    return pd.read_csv(path, usecols=usecols, dtype=dtype, low_memory=False)\n","\n","# === 2) ì½”ì–´ í…Œì´ë¸”: í™˜ì/ì…ì› =================================================\n","print(\"ğŸ“¥ Loading patients/admissions ...\")\n","patients = safe_read_csv(\n","    f\"{HOSP_DIR}/patients.csv\",\n","    usecols=[\"subject_id\",\"gender\",\"anchor_age\",\"anchor_year\",\"anchor_year_group\"]\n",")\n","\n","admissions = safe_read_csv(\n","    f\"{HOSP_DIR}/admissions.csv\",\n","    usecols=[\n","        \"subject_id\",\"hadm_id\",\"admittime\",\"dischtime\",\"deathtime\",\"admission_type\",\n","        \"admit_provider_id\",\"admission_location\",\"discharge_location\",\"insurance\",\n","        \"edregtime\",\"edouttime\",\"hospital_expire_flag\"\n","    ]\n",")\n","\n","for c in [\"admittime\",\"dischtime\",\"deathtime\",\"edregtime\",\"edouttime\"]:\n","    admissions[c] = td(admissions[c])\n","\n","core = admissions.merge(patients, on=\"subject_id\", how=\"left\")\n","\n","if HADM_LIMIT is not None:\n","    keep_hadm = core[\"hadm_id\"].dropna().unique()[:int(HADM_LIMIT)]\n","    core = core[core[\"hadm_id\"].isin(keep_hadm)].copy()\n","\n","print(f\"âœ… core rows: {len(core):,}\")\n","\n","# === 3) (A ë°©ì‹) í‡´ì›ìš”ì•½ í…ìŠ¤íŠ¸/ì„¹ì…˜ ì™„ì „ ìŠ¤í‚µ ================================\n","# (ì•„ë¬´ ì‘ì—…ë„ í•˜ì§€ ì•ŠìŒ: ë…¸íŠ¸ í…ìŠ¤íŠ¸/ì„¹ì…˜ì€ ìƒì„± ëª©í‘œì— ëˆ„ì„¤ì´ë¯€ë¡œ ì œì™¸)\n","\n","# === 4) ì²˜ë°©/EMAR ìš”ì•½(êµ¬ì¡°í™”) ================================================\n","print(\"ğŸ“¥ Loading prescriptions ...\")\n","presc_cols = [\n","    \"subject_id\",\"hadm_id\",\"pharmacy_id\",\"poe_id\",\"poe_seq\",\"order_provider_id\",\n","    \"starttime\",\"stoptime\",\"drug_type\",\"drug\",\"formulary_drug_cd\",\"gsn\",\"ndc\",\n","    \"prod_strength\",\"form_rx\",\"dose_val_rx\",\"dose_unit_rx\",\n","    \"form_val_disp\",\"form_unit_disp\",\"doses_per_24_hrs\",\"route\"\n","]\n","presc = safe_read_csv(f\"{HOSP_DIR}/prescriptions.csv\", usecols=presc_cols)\n","presc[\"starttime\"] = td(presc[\"starttime\"])\n","presc[\"stoptime\"]  = td(presc[\"stoptime\"])\n","\n","# í˜„ì¬ core hadmë§Œ + ì…ì›ì°½ ê²¹ì¹¨\n","adm_times = core.set_index(\"hadm_id\")[[\"admittime\",\"dischtime\"]].to_dict(\"index\")\n","presc = presc[presc[\"hadm_id\"].isin(core[\"hadm_id\"])].copy()\n","\n","def overlaps_adm(row):\n","    a = adm_times.get(row[\"hadm_id\"])\n","    if a is None: return False\n","    s, e = row[\"starttime\"], row[\"stoptime\"]\n","    if pd.isna(s) or pd.isna(e) or pd.isna(a[\"admittime\"]) or pd.isna(a[\"dischtime\"]):\n","        return True\n","    return not (e < a[\"admittime\"] or s > a[\"dischtime\"])\n","\n","presc = presc[presc.apply(overlaps_adm, axis=1)]\n","\n","def summarize_prescriptions(df):\n","    def one_hadm(g):\n","        items = []\n","        for _, r in g.iterrows():\n","            items.append({\n","                \"drug\": r[\"drug\"],\n","                \"route\": r[\"route\"],\n","                \"dose\": (str(r[\"dose_val_rx\"]) if pd.notna(r[\"dose_val_rx\"]) else None),\n","                \"dose_unit\": r[\"dose_unit_rx\"],\n","                \"doses_per_24_hrs\": r[\"doses_per_24_hrs\"],\n","                \"start\": (r[\"starttime\"].isoformat() if pd.notna(r[\"starttime\"]) else None),\n","                \"stop\":  (r[\"stoptime\"].isoformat() if pd.notna(r[\"stoptime\"]) else None),\n","                \"drug_type\": r[\"drug_type\"],\n","            })\n","        return jdump(items)\n","    return (df.groupby(\"hadm_id\", group_keys=False)\n","              .apply(lambda g: pd.Series({\"inpatient_med_summary_json\": one_hadm(g)}))\n","              .reset_index())\n","\n","presc_summary = summarize_prescriptions(presc)\n","\n","# eMAR(ì„ íƒ)\n","if USE_EMAR:\n","    print(\"ğŸ“¥ Loading eMAR ...\")\n","    emar = safe_read_csv(\n","        f\"{HOSP_DIR}/emar.csv\",\n","        usecols=[\"subject_id\",\"hadm_id\",\"emar_id\",\"emar_seq\",\"poe_id\",\"pharmacy_id\",\"enter_provider_id\",\"charttime\",\"medication\",\"event_txt\",\"scheduletime\",\"storetime\"]\n","    )\n","    emar[\"charttime\"] = td(emar[\"charttime\"])\n","    emar = emar[emar[\"hadm_id\"].isin(core[\"hadm_id\"])].copy()\n","\n","    def summarize_emar(df):\n","        def one_hadm(g):\n","            out = {}\n","            for med, gg in g.groupby(\"medication\"):\n","                d = {\n","                    \"n_events\": int(len(gg)),\n","                    \"n_given\": int((gg[\"event_txt\"].str.lower()==\"given\").sum()),\n","                    \"last_charttime\": (gg[\"charttime\"].max().isoformat() if pd.notna(gg[\"charttime\"].max()) else None),\n","                }\n","                out[med] = d\n","            return jdump(out)\n","        return (df.groupby(\"hadm_id\", group_keys=False)\n","                  .apply(lambda g: pd.Series({\"emar_admin_summary_json\": one_hadm(g)}))\n","                  .reset_index())\n","    emar_summary = summarize_emar(emar)\n","else:\n","    emar_summary = pd.DataFrame(columns=[\"hadm_id\",\"emar_admin_summary_json\"])\n","\n","# === 5) Labs/Micro (êµ¬ì¡°í™”ë§Œ) ==================================================\n","if USE_LABS:\n","    print(\"ğŸ“¥ Loading d_labitems & labevents (filtered) ...\")\n","    dlab = safe_read_csv(f\"{HOSP_DIR}/d_labitems.csv\", usecols=[\"itemid\",\"label\",\"fluid\",\"category\"])\n","    TARGET_LABELS = {\n","        \"WBC\":\"WBC\",\"Hemoglobin\":\"Hgb\",\"Platelet Count\":\"Plt\",\n","        \"Sodium\":\"Na\",\"Potassium\":\"K\",\"Chloride\":\"Cl\",\"Bicarbonate\":\"HCO3\",\n","        \"Creatinine\":\"Cr\",\"Urea Nitrogen\":\"BUN\",\"Glucose\":\"Glucose\",\n","    }\n","    target_dlab = dlab[dlab[\"label\"].isin(TARGET_LABELS.keys())].copy()\n","    target_dlab[\"short\"] = target_dlab[\"label\"].map(TARGET_LABELS)\n","    target_itemids = set(target_dlab[\"itemid\"].tolist())\n","    target_map     = dict(zip(target_dlab[\"itemid\"], target_dlab[\"short\"]))\n","\n","    lab_summary_rows = []\n","    cols = [\"labevent_id\",\"subject_id\",\"hadm_id\",\"specimen_id\",\"itemid\",\"charttime\",\"value\",\"valuenum\",\"valueuom\",\"ref_range_lower\",\"ref_range_upper\",\"flag\",\"priority\"]\n","    chunk_iter = pd.read_csv(f\"{HOSP_DIR}/labevents.csv\", usecols=cols, chunksize=1_000_000, low_memory=False)\n","    keep_hadm_set = set(core[\"hadm_id\"].dropna().unique())\n","    for chunk in chunk_iter:\n","        chunk = chunk[chunk[\"hadm_id\"].isin(keep_hadm_set)]\n","        chunk = chunk[chunk[\"itemid\"].isin(target_itemids)]\n","        if chunk.empty:\n","            continue\n","        chunk[\"charttime\"] = td(chunk[\"charttime\"])\n","        def in_window(r):\n","            a = adm_times.get(r[\"hadm_id\"])\n","            if a is None: return False\n","            ct = r[\"charttime\"]\n","            if pd.isna(ct) or pd.isna(a[\"admittime\"]) or pd.isna(a[\"dischtime\"]):\n","                return True\n","            return (a[\"admittime\"] <= ct) and (ct <= a[\"dischtime\"])\n","        chunk = chunk[chunk.apply(in_window, axis=1)]\n","        if chunk.empty:\n","            continue\n","        chunk[\"short\"] = chunk[\"itemid\"].map(target_map)\n","        agg = (chunk.sort_values(\"charttime\")\n","                    .groupby([\"hadm_id\",\"short\"])\n","                    .agg(min_val=(\"valuenum\",\"min\"),\n","                         max_val=(\"valuenum\",\"max\"),\n","                         last_val=(\"valuenum\",\"last\"),\n","                         unit=(\"valueuom\",\"last\"))\n","                    .reset_index())\n","        lab_summary_rows.append(agg); del chunk, agg; gc.collect()\n","\n","    if lab_summary_rows:\n","        labs_agg = pd.concat(lab_summary_rows, ignore_index=True)\n","        def one_hadm(g):\n","            out = {}\n","            for _, r in g.iterrows():\n","                out[r[\"short\"]] = {\n","                    \"min\": (None if pd.isna(r[\"min_val\"]) else float(r[\"min_val\"])),\n","                    \"max\": (None if pd.isna(r[\"max_val\"]) else float(r[\"max_val\"])),\n","                    \"last\":(None if pd.isna(r[\"last_val\"]) else float(r[\"last_val\"])),\n","                    \"unit\": r[\"unit\"]\n","                }\n","            return jdump(out)\n","        labs_summary_full = (labs_agg.groupby(\"hadm_id\", group_keys=False)\n","                               .apply(lambda g: pd.Series({\"lab_summary_json\": one_hadm(g)}))\n","                               .reset_index())\n","    else:\n","        labs_summary_full = pd.DataFrame(columns=[\"hadm_id\",\"lab_summary_json\"])\n","else:\n","    labs_summary_full = pd.DataFrame(columns=[\"hadm_id\",\"lab_summary_json\"])\n","\n","# (ì„ íƒ) A-ì…ë ¥ ê²½ëŸ‰í™”: labsë¥¼ last+unitë§Œ ë‚¨ê¸°ëŠ” í•¨ìˆ˜\n","def labs_last_only(js):\n","    if pd.isna(js) or js is None:\n","        return None\n","    try:\n","        obj = json.loads(js) if isinstance(js, str) else js\n","        out = {}\n","        for k, d in obj.items():\n","            if not isinstance(d, dict):\n","                continue\n","            last = d.get(\"last\")\n","            unit = d.get(\"unit\")\n","            if last is not None:\n","                out[k] = {\"last\": last, \"unit\": unit}\n","        return jdump(out)\n","    except Exception:\n","        return None\n","\n","if USE_LABS:\n","    labs_summary = labs_summary_full.copy()\n","    labs_summary[\"lab_summary_json\"] = labs_summary[\"lab_summary_json\"].apply(labs_last_only)\n","else:\n","    labs_summary = labs_summary_full\n","\n","# Microbiology (êµ¬ì¡°í™” JSON)\n","if USE_MICROBIO:\n","    print(\"ğŸ“¥ Loading microbiologyevents ...\")\n","    micro = safe_read_csv(\n","        f\"{HOSP_DIR}/microbiologyevents.csv\",\n","        usecols=[\"microevent_id\",\"subject_id\",\"hadm_id\",\"micro_specimen_id\",\"chartdate\",\"charttime\",\n","                 \"spec_itemid\",\"spec_type_desc\",\"test_itemid\",\"test_name\",\"org_itemid\",\"org_name\",\n","                 \"isolate_num\",\"quantity\",\"ab_itemid\",\"ab_name\",\"dilution_text\",\"dilution_comparison\",\"dilution_value\",\"interpretation\",\"comments\"]\n","    )\n","    micro[\"charttime\"] = td(micro[\"charttime\"])\n","    micro = micro[micro[\"hadm_id\"].isin(core[\"hadm_id\"])].copy()\n","\n","    def summarize_micro(g):\n","        res = []\n","        for (spec, org), gg in g.groupby([\"spec_type_desc\",\"org_name\"], dropna=False):\n","            inter = gg[\"interpretation\"].dropna().value_counts().to_dict()\n","            res.append({\"specimen\": spec, \"organism\": org, \"n\": int(len(gg)), \"interpretation_cnt\": inter})\n","        return jdump(res)\n","    micro_summary = (micro.groupby(\"hadm_id\", group_keys=False)\n","                        .apply(lambda g: pd.Series({\"microbio_summary_json\": summarize_micro(g)}))\n","                        .reset_index())\n","else:\n","    micro_summary = pd.DataFrame(columns=[\"hadm_id\",\"microbio_summary_json\"])\n","\n","# === 6) ICU ê²½ëŸ‰ ==============================================================\n","if USE_ICU_LIGHT:\n","    print(\"ğŸ“¥ Loading icustays (light) ...\")\n","    icu = safe_read_csv(\n","        f\"{ICU_DIR}/icustays.csv\",\n","        usecols=[\"subject_id\",\"hadm_id\",\"stay_id\",\"first_careunit\",\"last_careunit\",\"intime\",\"outtime\",\"los\"]\n","    )\n","    for c in [\"intime\",\"outtime\"]:\n","        icu[c] = td(icu[c])\n","    icu_agg = (icu.groupby(\"hadm_id\", as_index=False)\n","                  .agg(icu_stay_count=(\"stay_id\",\"count\"),\n","                       icu_first_intime=(\"intime\",\"min\"),\n","                       icu_last_outtime=(\"outtime\",\"max\"),\n","                       icu_total_los_days=(\"los\",\"sum\")))\n","else:\n","    icu_agg = pd.DataFrame(columns=[\"hadm_id\",\"icu_stay_count\",\"icu_first_intime\",\"icu_last_outtime\",\"icu_total_los_days\"])\n","\n","# === 7) ì¡°ë¦½ & A-ì…ë ¥ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ===========================================\n","print(\"ğŸ§© Assembling (A-input) ...\")\n","wide = (core\n","    .merge(presc_summary, on=\"hadm_id\", how=\"left\")\n","    .merge(emar_summary, on=\"hadm_id\", how=\"left\")\n","    .merge(labs_summary, on=\"hadm_id\", how=\"left\")\n","    .merge(micro_summary, on=\"hadm_id\", how=\"left\")\n","    .merge(icu_agg, on=\"hadm_id\", how=\"left\")\n",")\n","\n","# ë‚ ì§œ ë¬¸ìì—´í™”\n","date_cols = [\"admittime\",\"dischtime\",\"deathtime\",\"edregtime\",\"edouttime\",\"icu_first_intime\",\"icu_last_outtime\"]\n","for c in date_cols:\n","    if c in wide.columns:\n","        wide[c] = wide[c].dt.tz_convert(None).astype(str).replace(\"NaT\",\"\")\n","\n","# âœ… A-ì…ë ¥ì— ë‚¨ê¸¸ ì»¬ëŸ¼(êµ¬ì¡°í™”ë§Œ)\n","INPUT_KEEP = [\n","    \"subject_id\",\"hadm_id\",\n","    \"gender\",\"anchor_age\",\"anchor_year_group\",\n","    \"admission_type\",\"admission_location\",\"discharge_location\",\"insurance\",\n","    \"admittime\",\"dischtime\",\"hospital_expire_flag\",\n","    \"icu_stay_count\",\"icu_total_los_days\",\n","    \"inpatient_med_summary_json\",\n","    \"lab_summary_json\",\n","    \"microbio_summary_json\",\n","]\n","if USE_EMAR:\n","    INPUT_KEEP.append(\"emar_admin_summary_json\")\n","\n","input_A = wide[[c for c in INPUT_KEEP if c in wide.columns]].copy()\n","\n","print(f\"âœ… A-input rows: {len(input_A):,}, cols: {len(input_A.columns)}\")\n","in_base = os.path.join(OUTPUT_DIR, \"mimiciv_record_sheet\")\n","\n","if SAVE_PARQUET:\n","    input_A.to_parquet(in_base + \".parquet\", index=False)\n","    print(f\"ğŸ’¾ Saved Parquet â†’ {in_base}.parquet\")\n","if SAVE_CSV:\n","    input_A.to_csv(in_base + \".csv\", index=False)\n","    print(f\"ğŸ’¾ Saved CSV     â†’ {in_base}.csv\")\n","\n","print(\"ğŸ‰ Done (A-input)!\")\n"]},{"cell_type":"markdown","metadata":{"id":"PqvPtAQJ-vkw"},"source":["# 2. ì¶œë ¥(í‡´ì› ìš”ì•½) ë°ì´í„°ì…‹ íŒŒì¼ ì €ì¥"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AVPveuFK-1iD","outputId":"b2d1250f-b76c-4bb5-f87a-2016642abefe"},"outputs":[{"name":"stdout","output_type":"stream","text":["âœ… rows: 3 | cols: 3\n","ğŸ’¾ Saved: /content/drive/MyDrive/DILAB/MARS/mimic-iv_reconstructed/discharge_records_A_output.parquet\n"]}],"source":["# === Discharge ì „ìš© (output: discharge_record_mdë§Œ) =========================\n","import os, re, json\n","import pandas as pd\n","import numpy as np\n","\n","# ê²½ë¡œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","NOTE_DIR   = \"/content/drive/MyDrive/DILAB/MARS/mimic-iv-note_2.2/files/note\"\n","OUTPUT_DIR = \"/content/drive/MyDrive/DILAB/MARS/mimic-iv_reconstructed\"\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","# ğŸ”— ì…ë ¥ íŒŒì¼ê³¼ ë§¤ì¹­(ìˆìœ¼ë©´ í•´ë‹¹ hadmë§Œ)\n","MASTER_PARQUET_PATH = os.path.join(OUTPUT_DIR, \"mimiciv_record_sheet.parquet\")\n","FILTER_TO_MASTER_HADM = True\n","\n","OUT_PARQUET = os.path.join(OUTPUT_DIR, \"mimiciv_note_record_sheet.parquet\")\n","OUT_CSV     = os.path.join(OUTPUT_DIR, \"mimiciv_note_record_sheet.csv\")\n","SAVE_CSV    = False\n","\n","def td(x): return pd.to_datetime(x, errors=\"coerce\", utc=True)\n","\n","# ì„¹ì…˜ íŒŒì‹±(ë°±ì—…ìš©) íŒ¨í„´ (ì •ê·œì‹ ì˜¤íƒˆì ìˆ˜ì •: {0,60})\n","SECTION_PATTERNS = [\n","    (\"chief_complaint\", r\"(?:^|\\n)\\s*(?:chief complaint|cc)\\s*[:\\-]\\s*(.*?)(?=\\n[A-Z][^\\n]{0,60}\\s*:|\\Z)\"),\n","    (\"hpi\",              r\"(?:^|\\n)\\s*(?:history of present illness|hpi)\\s*[:\\-]\\s*(.*?)(?=\\n[A-Z][^\\n]{0,60}\\s*:|\\Z)\"),\n","    (\"pmh\",              r\"(?:^|\\n)\\s*(?:past medical history|pmh)\\s*[:\\-]\\s*(.*?)(?=\\n[A-Z][^\\n]{0,60}\\s*:|\\Z)\"),\n","    (\"fhx\",              r\"(?:^|\\n)\\s*(?:family history)\\s*[:\\-]\\s*(.*?)(?=\\n[A-Z][^\\n]{0,60}\\s*:|\\Z)\"),\n","    (\"shx\",              r\"(?:^|\\n)\\s*(?:social history)\\s*[:\\-]\\s*(.*?)(?=\\n[A-Z][^\\n]{0,60}\\s*:|\\Z)\"),\n","    (\"allergies\",        r\"(?:^|\\n)\\s*(?:allergies?)\\s*[:\\-]\\s*(.*?)(?=\\n[A-Z][^\\n]{0,60}\\s*:|\\Z)\"),\n","    (\"physical_exam\",    r\"(?:^|\\n)\\s*(?:physical (?:exam|examination))\\s*[:\\-]\\s*(.*?)(?=\\n[A-Z][^\\n]{0,60}\\s*:|\\Z)\"),\n","    (\"assessment\",       r\"(?:^|\\n)\\s*(?:assessment|impression)\\s*[:\\-]\\s*(.*?)(?=\\n[A-Z][^\\n]{0,60}\\s*:|\\Z)\"),\n","    (\"hospital_course\",  r\"(?:^|\\n)\\s*(?:hospital course)\\s*[:\\-]\\s*(.*?)(?=\\n[A-Z][^\\n]{0,60}\\s*:|\\Z)\"),\n","    (\"discharge_diagnosis\", r\"(?:^|\\n)\\s*(?:discharge diagnosis(?:es)?)\\s*[:\\-]\\s*(.*?)(?=\\n[A-Z][^\\n]{0,60}\\s*:|\\Z)\"),\n","    (\"plan\",             r\"(?:^|\\n)\\s*(?:plan|discharge plan)\\s*[:\\-]\\s*(.*?)(?=\\n[A-Z][^\\n]{0,60}\\s*:|\\Z)\"),\n","    (\"discharge_instructions\", r\"(?:^|\\n)\\s*(?:discharge instructions?)\\s*[:\\-]\\s*(.*?)(?=\\n[A-Z][^\\n]{0,60}\\s*:|\\Z)\"),\n","    (\"meds_on_discharge\",r\"(?:^|\\n)\\s*(?:medications? on discharge|discharge medications?)\\s*[:\\-]\\s*(.*?)(?=\\n[A-Z][^\\n]{0,60}\\s*:|\\Z)\"),\n","]\n","SECTION_COLS = [\n","    \"chief_complaint\",\"hpi\",\"pmh\",\"fhx\",\"shx\",\"allergies\",\n","    \"physical_exam\",\"assessment\",\"hospital_course\",\"discharge_diagnosis\",\n","    \"plan\",\"discharge_instructions\",\"meds_on_discharge\",\"assessment_plan\"\n","]\n","LABELS = {\n","    \"chief_complaint\":\"ì£¼í˜¸ì†Œ\",\"hpi\":\"í˜„ë³‘ë ¥\",\"pmh\":\"ê³¼ê±°ë ¥\",\"fhx\":\"ê°€ì¡±ë ¥\",\"shx\":\"ì‚¬íšŒë ¥\",\n","    \"allergies\":\"ì•Œë ˆë¥´ê¸°\",\"physical_exam\":\"ì‹ ì²´ê²€ì§„\",\"assessment\":\"í‰ê°€/ì¸ìƒ\",\n","    \"hospital_course\":\"ì…ì›ê²½ê³¼\",\"discharge_diagnosis\":\"í‡´ì›ì§„ë‹¨\",\n","    \"plan\":\"ì¹˜ë£Œê³„íš\",\"discharge_instructions\":\"í‡´ì›ì§€ì‹œ\",\"meds_on_discharge\":\"í‡´ì›ì•½\",\n","    \"assessment_plan\":\"í‰ê°€/ê³„íš\"\n","}\n","\n","def fallback_parse_sections(text: str) -> dict:\n","    if not isinstance(text, str) or not text.strip():\n","        return {}\n","    t = text.lower()\n","    out = {}\n","    for key, pat in SECTION_PATTERNS:\n","        m = re.search(pat, t, flags=re.S|re.I)\n","        if m:\n","            out[key] = m.group(1).strip()\n","    return out\n","\n","def compose_discharge_md(row: pd.Series) -> str:\n","    parts = [f\"# í‡´ì›ê¸°ë¡ì§€ (subject_id={row['subject_id']}, hadm_id={row['hadm_id']})\"]\n","    for k in [\n","        \"chief_complaint\",\"hpi\",\"pmh\",\"fhx\",\"shx\",\"allergies\",\n","        \"physical_exam\",\"assessment\",\"hospital_course\",\"discharge_diagnosis\",\n","        \"plan\",\"discharge_instructions\",\"meds_on_discharge\"\n","    ]:\n","        val = row.get(k)\n","        if pd.notna(val) and str(val).strip():\n","            parts.append(f\"## {LABELS.get(k,k)}\\n{str(val).strip()}\")\n","    return \"\\n\\n\".join(parts)\n","\n","# 1) discharge ì›ë¬¸ ë¡œë“œ\n","dis = pd.read_csv(\n","    os.path.join(NOTE_DIR, \"discharge.csv\"),\n","    usecols=[\"note_id\",\"subject_id\",\"hadm_id\",\"note_type\",\"note_seq\",\"charttime\",\"storetime\",\"text\"],\n","    low_memory=False\n",")\n","dis[\"charttime\"] = td(dis[\"charttime\"]); dis[\"storetime\"] = td(dis[\"storetime\"])\n","\n","# (ì˜µì…˜) A-ì…ë ¥ê³¼ hadm ë™ê¸°í™”\n","if FILTER_TO_MASTER_HADM and os.path.exists(MASTER_PARQUET_PATH):\n","    master = pd.read_parquet(MASTER_PARQUET_PATH, columns=[\"hadm_id\"])\n","    keep = set(master[\"hadm_id\"].dropna().unique().tolist())\n","    dis = dis[dis[\"hadm_id\"].isin(keep)].copy()\n","\n","# hadmë³„ ìµœì‹ ë³¸\n","dis_latest = (dis.sort_values([\"hadm_id\",\"note_seq\",\"charttime\"])\n","                .drop_duplicates(subset=[\"hadm_id\"], keep=\"last\")\n","                .reset_index(drop=True))\n","\n","# 2) detailë¡œ ì„¹ì…˜ ì±„ìš°ê¸°(ê°€ëŠ¥í•˜ë©´)\n","detail_path = os.path.join(NOTE_DIR, \"discharge_detail.csv\")\n","if os.path.exists(detail_path):\n","    det = pd.read_csv(\n","        detail_path,\n","        usecols=[\"note_id\",\"subject_id\",\"field_name\",\"field_value\",\"field_ordinal\"],\n","        low_memory=False\n","    ).sort_values([\"note_id\",\"field_name\",\"field_ordinal\"])\n","\n","    agg = (det.groupby([\"note_id\",\"field_name\"], as_index=False)[\"field_value\"]\n","              .apply(lambda s: \"\\n\".join([str(x) for x in s if pd.notna(x)])))\n","\n","    FIELD_MAP = {\n","        \"Chief Complaint\":\"chief_complaint\",\n","        \"History of Present Illness\":\"hpi\",\n","        \"Past Medical History\":\"pmh\",\n","        \"Family History\":\"fhx\",\n","        \"Social History\":\"shx\",\n","        \"Allergies\":\"allergies\",\n","        \"Physical Exam\":\"physical_exam\",\n","        \"Assessment\":\"assessment\",\n","        \"Impression\":\"assessment\",\n","        \"Hospital Course\":\"hospital_course\",\n","        \"Discharge Diagnosis\":\"discharge_diagnosis\",\n","        \"Plan\":\"plan\",\n","        \"Assessment and Plan\":\"assessment_plan\",\n","        \"Discharge Instructions\":\"discharge_instructions\",\n","        \"Medications on Discharge\":\"meds_on_discharge\",\n","        \"Discharge Medications\":\"meds_on_discharge\",\n","    }\n","    agg[\"std_key\"] = agg[\"field_name\"].map(FIELD_MAP).fillna(agg[\"field_name\"])\n","    piv = agg.pivot(index=\"note_id\", columns=\"std_key\", values=\"field_value\").reset_index()\n","    dis_latest = dis_latest.merge(piv, on=\"note_id\", how=\"left\")\n","\n","# ì„¹ì…˜ dtype ì •ë¦¬\n","for col in SECTION_COLS:\n","    if col not in dis_latest.columns:\n","        dis_latest[col] = pd.Series(index=dis_latest.index, dtype=\"string\")\n","    else:\n","        dis_latest[col] = dis_latest[col].astype(\"string\")\n","\n","# 3) detail ë¹„ì–´ìˆìœ¼ë©´ ì›ë¬¸ì—ì„œ ë°±ì—… íŒŒì‹±\n","mask_need = dis_latest[SECTION_COLS].isna().all(axis=1)\n","if mask_need.any():\n","    parsed_df = (pd.json_normalize(dis_latest.loc[mask_need, \"text\"].apply(fallback_parse_sections))\n","                   .reindex(columns=SECTION_COLS).astype(\"string\"))\n","    parsed_df.index = dis_latest.index[mask_need]\n","    sub = dis_latest.loc[mask_need, SECTION_COLS]\n","    dis_latest.loc[mask_need, SECTION_COLS] = sub.where(sub.notna(), parsed_df)\n","\n","# 4) MD í•œ ì¥ ìƒì„±\n","dis_latest[\"discharge_record_md\"] = dis_latest.apply(compose_discharge_md, axis=1)\n","\n","# 5) ğŸ”¥ A-ì¶œë ¥ ìµœì†Œ ì»¬ëŸ¼ë§Œ ì €ì¥\n","out_A = dis_latest[[\"subject_id\",\"hadm_id\",\"discharge_record_md\"]].copy()\n","\n","out_A.to_parquet(OUT_PARQUET, index=False)\n","if SAVE_CSV:\n","    out_A.to_csv(OUT_CSV, index=False)\n","\n","print(f\"âœ… rows: {len(out_A):,} | cols: {len(out_A.columns)}\")\n","print(f\"ğŸ’¾ Saved: {OUT_PARQUET}\")\n","if SAVE_CSV:\n","    print(f\"ğŸ’¾ Saved: {OUT_CSV}\")\n"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyNOKRLfSr0MXuE/Y64FIpVG"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}